{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t2IuTyp8mAc"
   },
   "source": [
    "# Model interpretation for Visual Question Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF6NqYOL8mAe"
   },
   "source": [
    "In this notebook we demonstrate how to apply model interpretability algorithms from captum library on VQA models. More specifically we explain model predictions by applying integrated gradients on a small sample of image-question pairs. More details about Integrated gradients can be found in the original paper: https://arxiv.org/pdf/1703.01365.pdf\n",
    "\n",
    "As a reference VQA model we use the following open source implementation:\n",
    "https://github.com/Cyanogenoid/pytorch-vqa\n",
    "  \n",
    "  **Note:** Before running this tutorial, please install the `torchvision`, `PIL`, and `matplotlib` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B6dgtfc9YsV",
    "outputId": "8e9a738c-8829-44bd-d00c-b0bebd7b14fb"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Cyanogenoid/pytorch-vqa.git\n",
    "# !git clone https://github.com/Cyanogenoid/pytorch-resnet.git\n",
    "# !wget 'https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jv8xYYLw8mAe"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(\"pytorch-vqa/\")\n",
    "sys.path.append(\"pytorch-resnet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ETngSwNA8mAg"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import resnet  # from pytorch-resnet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from model import Net, apply_attention, tile_2d_over_nd # from pytorch-vqa\n",
    "from utils import get_transform # from pytorch-vqa\n",
    "\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    LayerIntegratedGradients,\n",
    "    TokenReferenceBase,\n",
    "    configure_interpretable_embedding_layer,\n",
    "    remove_interpretable_embedding_layer,\n",
    "    visualization\n",
    ")\n",
    "from captum.attr._utils.input_layer_wrapper import ModelInputWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jdD_QAf48mAg"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dq0gBXm8mAh"
   },
   "source": [
    "# Loading VQA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdLddrek8mAh"
   },
   "source": [
    "VQA model can be downloaded from: \n",
    "https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dB7mmJIu8mAi"
   },
   "outputs": [],
   "source": [
    "saved_state = torch.load('2017-08-04_00.55.19.pth', map_location=device)\n",
    "# reading vocabulary from saved model\n",
    "vocab = saved_state['vocab']\n",
    "\n",
    "# reading word tokens from saved model\n",
    "token_to_index = vocab['question']\n",
    "\n",
    "# reading answers from saved model\n",
    "answer_to_index = vocab['answer']\n",
    "\n",
    "num_tokens = len(token_to_index) + 1\n",
    "\n",
    "# reading answer classes from the vocabulary\n",
    "answer_words = ['unk'] * len(answer_to_index)\n",
    "for w, idx in answer_to_index.items():\n",
    "    answer_words[idx]=w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIOhZsS18mAi"
   },
   "source": [
    "Loads predefined VQA model and sets it to eval mode.\n",
    "`device_ids` contains a list of GPU ids which are used for paralelization supported by `DataParallel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-ibyh-y8mAj",
    "outputId": "cb9285d8-74bd-4fd3-a891-4c2460386255"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasoncyuan/vqa/pytorch-vqa/model.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.embedding.weight)\n",
      "/Users/jasoncyuan/vqa/pytorch-vqa/model.py:44: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Net(\n",
       "    (text): TextProcessor(\n",
       "      (embedding): Embedding(15193, 300, padding_idx=0)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (tanh): Tanh()\n",
       "      (lstm): LSTM(300, 1024)\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (v_conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (q_lin): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (x_conv): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (classifier): Classifier(\n",
       "      (drop1): Dropout(p=0.5, inplace=False)\n",
       "      (lin1): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (drop2): Dropout(p=0.5, inplace=False)\n",
       "      (lin2): Linear(in_features=1024, out_features=3000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_net = torch.nn.DataParallel(Net(num_tokens))\n",
    "vqa_net.load_state_dict(saved_state['weights'])\n",
    "vqa_net.to(device)\n",
    "vqa_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0gt6pU48mAj"
   },
   "source": [
    "Converting string question into a tensor. `encode_question` function is similar to original implementation of `encode_question` method in pytorch-vqa source code.\n",
    "https://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py#L110\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XthNgHXY8mAk"
   },
   "outputs": [],
   "source": [
    "def encode_question(question):\n",
    "    \"\"\" Turn a question into a vector of indices and a question length. Unrecognized turned into 0.\"\"\"\n",
    "    question_arr = question.lower().split()\n",
    "    vec = torch.zeros(len(question_arr), device=device).long()\n",
    "    for i, token in enumerate(question_arr):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, torch.tensor(len(question_arr), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR5KnIP48mAk"
   },
   "source": [
    "# Defining end-to-end VQA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTdjVGuP8mAk"
   },
   "source": [
    "Original saved model does not have image network's (resnet's) layers attached to it. We attach it in the below cell using forward-hook. The rest of the model is identical to the original definition of the model: https://github.com/Cyanogenoid/pytorch-vqa/blob/master/model.py#L48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "473l-zQ28mAl"
   },
   "outputs": [],
   "source": [
    "class ResNetLayer4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r_model = resnet.resnet50(pretrained=True) # [18, 34, 50, 101, 150]\n",
    "        self.r_model.eval()\n",
    "        self.r_model.to(device)\n",
    "\n",
    "        self.buffer = {}\n",
    "        lock = threading.Lock()\n",
    "\n",
    "        # Since we only use the output of the 4th layer from the resnet model and do not\n",
    "        # need to do forward pass all the way to the final layer we can terminate forward\n",
    "        # execution in the forward hook of that layer after obtaining the output of it.\n",
    "        # For that reason, we can define a custom Exception class that will be used for\n",
    "        # raising early termination error.\n",
    "        def save_output(module, input, output):\n",
    "            with lock:\n",
    "                self.buffer[output.device] = output\n",
    "\n",
    "        self.r_model.layer4.register_forward_hook(save_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.r_model(x)          \n",
    "        return self.buffer[x.device]\n",
    "\n",
    "class VQA_Resnet_Model(Net):\n",
    "    def __init__(self, embedding_tokens):\n",
    "        super().__init__(embedding_tokens)\n",
    "        self.resnet_layer4 = ResNetLayer4()\n",
    "    \n",
    "    def forward(self, v, q, q_len):\n",
    "        q = self.text(q, list(q_len.data))\n",
    "        v = self.resnet_layer4(v)\n",
    "\n",
    "        v = v / (v.norm(p=2, dim=1, keepdim=True).expand_as(v) + 1e-8)\n",
    "\n",
    "        a = self.attention(v, q)\n",
    "        v = apply_attention(v, a)\n",
    "\n",
    "        combined = torch.cat([v, q], dim=1)\n",
    "        answer = self.classifier(combined)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F41htrEN8mAl"
   },
   "source": [
    "In order to explain text features, we must let integrated gradients attribute on the embeddings, not the indices. The reason for this is simply due to Integrated Gradients being a gradient-based attribution method, as we are unable to compute gradients with respect to integers.\n",
    "\n",
    "Hence, we have two options:\n",
    "1. \"Patch\" the model's embedding layer and corresponding inputs. To patch the layer, use the `configure_interpretable_embedding_layer`^ method, which will wrap the associated layer you give it, with an identity function. This identity function accepts an embedding and outputs an embedding. You can patch the inputs, i.e. obtain the embedding for a set of indices, with `model.wrapped_layer.indices_to_embeddings(indices)`.\n",
    "2. Use the equivalent layer attribution algorithm (`LayerIntegratedGradients` in our case) with the utility class `ModelInputWrapper`. The `ModelInputWrapper` will wrap your model and feed all it's inputs to seperate layers; allowing you to use layer attribution methods on inputs. You can access the associated layer for input named `\"foo\"` via the `ModuleDict`: `wrapped_model.input_maps[\"foo\"]`.\n",
    "\n",
    "^ NOTE: For option (1), after finishing interpretation it is important to call `remove_interpretable_embedding_layer` which removes the Interpretable Embedding Layer that we added for interpretation purposes and sets the original embedding layer back in the model.\n",
    "\n",
    "Below I am using the `USE_INTEPRETABLE_EMBEDDING_LAYER` flag to do option (1) if it is True, otherwise (2) if it is False. Generally it is reccomended to do option (2) since this option is much more flexible and easy to use. The reason it is more flexible is it allows your model to do any sort of preprocessing to the indices tensor. It's easier to use since you don't have to touch your inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DCXsKytj8mAm"
   },
   "outputs": [],
   "source": [
    "USE_INTEPRETABLE_EMBEDDING_LAYER = False  # set to True for option (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpWZlTBY8mAm"
   },
   "source": [
    "Updating weights from the saved model and removing the old model from the memory. And wrap the model with `ModelInputWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161,
     "referenced_widgets": [
      "ce654dc2df9b4351ad054f6b9215a5a4",
      "bb0456ae676b4fe7b7fc88acea82875e",
      "6b9ae79836104134a116e6e8e574de3a",
      "e9149f4cdfbd4d0d9a3615e2598eb017",
      "9b137e3f96cd40b4befd8fc0ccd021f6",
      "70c0612ecc134426b31cad17783e647a",
      "621eb788876541028c0ec2449c188e55",
      "2966732417384859b98077bfeb0e9c96",
      "3b13a8aca598484c8d23eec22e9e04b9",
      "57a4ee9618f64679979031bc53eafb06",
      "76b1992ed59d426baeb1a1b7212cb9c5"
     ]
    },
    "id": "UuFc6Se48mAm",
    "outputId": "0d29c611-6511-4e27-8f1c-7796a819470c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://s3.amazonaws.com/pytorch/models/resnet50-19c8e357.pth\" to /Users/jasoncyuan/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "vqa_resnet = VQA_Resnet_Model(vqa_net.module.text.embedding.num_embeddings)\n",
    "\n",
    "# wrap the inputs into layers incase we wish to use a layer method\n",
    "vqa_resnet = ModelInputWrapper(vqa_resnet)\n",
    "\n",
    "# `device_ids` contains a list of GPU ids which are used for paralelization supported by `DataParallel`\n",
    "vqa_resnet = torch.nn.DataParallel(vqa_resnet)\n",
    "\n",
    "# saved vqa model's parameters\n",
    "partial_dict = vqa_net.state_dict()\n",
    "\n",
    "state = vqa_resnet.module.state_dict()\n",
    "state.update(partial_dict)\n",
    "vqa_resnet.module.load_state_dict(state)\n",
    "\n",
    "vqa_resnet.to(device)\n",
    "vqa_resnet.eval()\n",
    "\n",
    "# This is original VQA model without resnet. Removing it, since we do not need it\n",
    "del vqa_net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaThuBlZ8mAm"
   },
   "source": [
    "Patch the model's embedding layer if we're doing option (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YzvkAG3q8mAm"
   },
   "outputs": [],
   "source": [
    "if USE_INTEPRETABLE_EMBEDDING_LAYER:\n",
    "    interpretable_embedding = configure_interpretable_embedding_layer(vqa_resnet, 'module.module.text.embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kTJxYHz8mAn"
   },
   "source": [
    "Below function will help us to transform and image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7yWugxh8mAn",
    "outputId": "2a219deb-efb4-48ca-d109-0a53b98e18fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasoncyuan/miniconda3/envs/captum/lib/python3.9/site-packages/torchvision/transforms/transforms.py:279: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "image_size = 448  # scale image to given size and center\n",
    "central_fraction = 1.0\n",
    "\n",
    "transform = get_transform(image_size, central_fraction=central_fraction)\n",
    "    \n",
    "def image_to_features(img):\n",
    "    img_transformed = transform(img)\n",
    "    img_batch = img_transformed.unsqueeze(0).to(device)\n",
    "    return img_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KsE4JB68mAn"
   },
   "source": [
    "Creating reference aka baseline / background for questions. This is specifically necessary for baseline-based model interpretability algorithms. In this case for integrated gradients. More details can be found in the original paper: https://arxiv.org/pdf/1703.01365.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Dkmuxsjz8mAn"
   },
   "outputs": [],
   "source": [
    "PAD_IND = token_to_index['pad']\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9Y-v6ROG8mAn"
   },
   "outputs": [],
   "source": [
    "# this is necessary for the backpropagation of RNNs models in eval mode\n",
    "torch.backends.cudnn.enabled=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIx21OJG8mAo"
   },
   "source": [
    "Creating an instance of layer integrated gradients for option (2); otherwise create an instance of integrated gradients for option (1). Both are equivalent methods to interpret the model's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSc40dXg8mAo",
    "outputId": "2647c16d-0939-4a59-8140-684cd6335cf9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasoncyuan/miniconda3/envs/captum/lib/python3.9/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:102: UserWarning: Multiple layers provided. Please ensure that each layer is**not** solely solely dependent on the outputs ofanother layer. Please refer to the documentation for moredetail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if USE_INTEPRETABLE_EMBEDDING_LAYER:\n",
    "    attr = IntegratedGradients(vqa_resnet)\n",
    "else:\n",
    "    attr = LayerIntegratedGradients(vqa_resnet, [vqa_resnet.module.input_maps[\"v\"], vqa_resnet.module.module.text.embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn-9gP3z8mAo"
   },
   "source": [
    "Defining default cmap that will be used for image visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZuhmvIxJ8mAo"
   },
   "outputs": [],
   "source": [
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#252b36'),\n",
    "                                                  (1, '#000000')], N=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlGz8n038mAo"
   },
   "source": [
    "Defining a few test images for model intepretation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "989Aj0Ke-lFn",
    "outputId": "09839ef7-ea66-4449-d8d3-031c33cbef06"
   },
   "outputs": [],
   "source": [
    "# !wget https://nitrocdn.com/mwIJloVUffDtKiCgRcivopdgojcJrVwT/assets/static/optimized/rev-3131a8b/image/siamese-cat-cover.jpg -O siamese.jpg\n",
    "# !wget https://s.abcnews.com/images/International/MChanga_1624536553868_hpMain_4x5_992.jpg -O elephant.jpg\n",
    "# !wget https://cdn.mos.cms.futurecdn.net/HjFE8NKWuCmgfHCcndJ3rK-480-80.jpg -O zebra.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "c_BiBoHp8mAp"
   },
   "outputs": [],
   "source": [
    "images = ['siamese.jpg',\n",
    "          'elephant.jpg',\n",
    "          'zebra.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "45POoZzb8mAp"
   },
   "outputs": [],
   "source": [
    "def vqa_resnet_interpret(image_filename, questions, targets):\n",
    "    img = Image.open(image_filename).convert('RGB')\n",
    "    original_image = transforms.Compose([transforms.Scale(int(image_size / central_fraction)),\n",
    "                                   transforms.CenterCrop(image_size), transforms.ToTensor()])(img) \n",
    "    \n",
    "    image_features = image_to_features(img).requires_grad_().to(device)\n",
    "    for question, target in zip(questions, targets):\n",
    "        q, q_len = encode_question(question)\n",
    "        \n",
    "        # generate reference for each sample\n",
    "        q_reference_indices = token_reference.generate_reference(q_len.item(), device=device).unsqueeze(0)\n",
    "\n",
    "        inputs = (q.unsqueeze(0), q_len.unsqueeze(0))\n",
    "        if USE_INTEPRETABLE_EMBEDDING_LAYER:\n",
    "            q_input_embedding = interpretable_embedding.indices_to_embeddings(q).unsqueeze(0)\n",
    "            q_reference_baseline = interpretable_embedding.indices_to_embeddings(q_reference_indices).to(device)\n",
    "\n",
    "            inputs = (image_features, q_input_embedding)\n",
    "            baselines = (image_features * 0.0, q_reference_baseline)\n",
    "            \n",
    "        else:            \n",
    "            inputs = (image_features, q.unsqueeze(0))\n",
    "            baselines = (image_features * 0.0, q_reference_indices)\n",
    "            \n",
    "        ans = vqa_resnet(*inputs, q_len.unsqueeze(0))\n",
    "            \n",
    "        # Make a prediction. The output of this prediction will be visualized later.\n",
    "        pred, answer_idx = F.softmax(ans, dim=1).data.cpu().max(dim=1)\n",
    "\n",
    "        attributions = attr.attribute(inputs=inputs,\n",
    "                                    baselines=baselines,\n",
    "                                    target=answer_idx,\n",
    "                                    additional_forward_args=q_len.unsqueeze(0),\n",
    "                                    n_steps=30)\n",
    "            \n",
    "        # Visualize text attributions\n",
    "        text_attributions_norm = attributions[1].sum(dim=2).squeeze(0).norm()\n",
    "        vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                attributions[1].sum(dim=2).squeeze(0) / text_attributions_norm,\n",
    "                                pred[0].item(),\n",
    "                                answer_words[ answer_idx ],\n",
    "                                answer_words[ answer_idx ],\n",
    "                                target,\n",
    "                                attributions[1].sum(),       \n",
    "                                question.split(),\n",
    "                                0.0)]\n",
    "        visualization.visualize_text(vis_data_records)\n",
    "\n",
    "        # visualize image attributions\n",
    "        original_im_mat = np.transpose(original_image.cpu().detach().numpy(), (1, 2, 0))\n",
    "        attributions_img = np.transpose(attributions[0].squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "        \n",
    "        visualization.visualize_image_attr_multiple(attributions_img, original_im_mat, \n",
    "                                                    [\"original_image\", \"heat_map\"], [\"all\", \"absolute_value\"], \n",
    "                                                    titles=[\"Original Image\", \"Attribution Magnitude\"],\n",
    "                                                    cmap=default_cmap,\n",
    "                                                    show_colorbar=True)\n",
    "        print('Text Contributions: ', attributions[1].sum().item())\n",
    "        print('Image Contributions: ', attributions[0].sum().item())\n",
    "        print('Total Contribution: ', attributions[0].sum().item() + attributions[1].sum().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMBHk1Up8mAp",
    "outputId": "a56e8ed6-cffa-4892-cfce-05140cd48711"
   },
   "outputs": [],
   "source": [
    "# the index of image in the test set. Please, change it if you want to play with different test images/samples.\n",
    "image_idx = 1 # elephant\n",
    "vqa_resnet_interpret(images[image_idx], [\n",
    "    \"what is on the picture\",\n",
    "    \"what color is the elephant\",\n",
    "    \"where is the elephant\"\n",
    "], ['elephant', 'gray', 'zoo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-XunmWh8mAp"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "# Above cell generates an output similar to this:\n",
    "IPython.display.Image(filename='img/vqa/elephant_attribution.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZ1GbDmc8mAq"
   },
   "outputs": [],
   "source": [
    "image_idx = 0 # cat\n",
    "\n",
    "vqa_resnet_interpret(images[image_idx], [\n",
    "    \"what is on the picture\",\n",
    "    \"what color are the cat's eyes\",\n",
    "    \"is the animal in the picture a cat or a fox\",\n",
    "    \"what color is the cat\",\n",
    "    \"how many ears does the cat have\",\n",
    "    \"where is the cat\"\n",
    "], ['cat', 'blue', 'cat', 'white and brown', '2', 'at the wall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DayoMhgB8mAr"
   },
   "outputs": [],
   "source": [
    "# Above cell generates an output similar to this:\n",
    "IPython.display.Image(filename='img/vqa/siamese_attribution.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_n9ce6f8mAr"
   },
   "outputs": [],
   "source": [
    "image_idx = 2 # zebra\n",
    "\n",
    "vqa_resnet_interpret(images[image_idx], [\n",
    "    \"what is on the picture\",\n",
    "    \"what color are the zebras\",\n",
    "    \"how many zebras are on the picture\",\n",
    "    \"where are the zebras\"\n",
    "], ['zebra', 'black and white', '2', 'zoo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7t2hvDy8mAr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Above cell generates an output similar to this:\n",
    "IPython.display.Image(filename='img/vqa/zebra_attribution.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Rw1S-0B8mAr"
   },
   "source": [
    "As mentioned above, after we are done with interpretation, we have to remove Interpretable Embedding Layer and set the original embeddings layer back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzUbd0sT8mAr"
   },
   "outputs": [],
   "source": [
    "if USE_INTEPRETABLE_EMBEDDING_LAYER:\n",
    "    remove_interpretable_embedding_layer(vqa_resnet, interpretable_embedding)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multimodal_VQA_Interpret (1).ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "fdeafbe1db737a37334a7a4556b1ce15b40105eb586a961a1579b06ea5e6a9ad"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2966732417384859b98077bfeb0e9c96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3b13a8aca598484c8d23eec22e9e04b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57a4ee9618f64679979031bc53eafb06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "621eb788876541028c0ec2449c188e55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b9ae79836104134a116e6e8e574de3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_621eb788876541028c0ec2449c188e55",
      "placeholder": "​",
      "style": "IPY_MODEL_70c0612ecc134426b31cad17783e647a",
      "value": "100%"
     }
    },
    "70c0612ecc134426b31cad17783e647a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76b1992ed59d426baeb1a1b7212cb9c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b137e3f96cd40b4befd8fc0ccd021f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76b1992ed59d426baeb1a1b7212cb9c5",
      "placeholder": "​",
      "style": "IPY_MODEL_57a4ee9618f64679979031bc53eafb06",
      "value": " 230M/230M [00:08&lt;00:00, 22.0MB/s]"
     }
    },
    "bb0456ae676b4fe7b7fc88acea82875e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce654dc2df9b4351ad054f6b9215a5a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b9ae79836104134a116e6e8e574de3a",
       "IPY_MODEL_e9149f4cdfbd4d0d9a3615e2598eb017",
       "IPY_MODEL_9b137e3f96cd40b4befd8fc0ccd021f6"
      ],
      "layout": "IPY_MODEL_bb0456ae676b4fe7b7fc88acea82875e"
     }
    },
    "e9149f4cdfbd4d0d9a3615e2598eb017": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b13a8aca598484c8d23eec22e9e04b9",
      "max": 241520640,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2966732417384859b98077bfeb0e9c96",
      "value": 241520640
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
